{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdP73PAAm6bu"
   },
   "source": [
    "# Neural Networks with TensorFlow and Keras\n",
    "\n",
    "Deep neural networks는 수십 혹은 수백개 layer가 될 수도 있다. \"deep\"이라는 단어가 여기서 유례했다고 할 수 있다. 이전 notebook에서 본바와 같이 weight 행렬만 가지고도 이런 deep networks들 중에 하나를 생성할 수 있다. 하지만 일반적으로 복잡하고 구현하기 어려워진다. TensorFlow는 **Keras** 라는 멋진 API를 가지고 있으며 효율적으로 대규모 neural networks를 만들 수 있는 멋진 방법을 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHb_h16-YOes"
   },
   "source": [
    "## Import Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVpuOzdonZdj"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NBOTTYzQVaLz",
    "outputId": "95fa3fdd-5870-4e39-99b2-2c3da82d5971"
   },
   "outputs": [],
   "source": [
    "print('Using:')\n",
    "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
    "print('\\t\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcHviD_uYQ5R"
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8cMpbPrngfy"
   },
   "source": [
    "이제 대규모 network를 만들꺼고 이 network는 어려운 문제를 해결하는데 image 내에 문자를 식별하는 것이다. 여기서 MNIST dataset을 사용하는데 이 데이터는 greyscale의 손으로 쓴 숫자로 구성되어 있다. 각 image는 28x28 pixels이고 아래와 같은 샘플을 볼 수 있을 것이다.\n",
    "\n",
    "<img src='assets/mnist.png'>\n",
    "\n",
    "neural network를 생성하는 우리의 목표는 이런 images들 중에 하나를 가져와서 이 image에 있는 숫자를 추측하는 것이다.\n",
    "\n",
    "먼저 dataset을 얻고 우리의 Neural Network을 train시키고 test해서 사용할 예정이다. [`tensorflow_datasets`](https://www.tensorflow.org/datasets) 패키지를 사용하여 dataset을 얻는다. TensorFlow Dataset은 TensorFlow와 함께 사용하기 위해 준비된 datasets의 저장소이다. TensorFlow Datasets은 다양한 서로 다른 tasks를 위한 머신러닝 모델을 train시키기 위해서 광범위한 datasets들ㅇ르 가지고 있다.(문자에서 비디오까지) dataset의 전체 목록은 TensorFlow Datasets인 [TensorFlow Datasets Catalog](https://www.tensorflow.org/datasets/catalog/overview#all_datasets)를 확인해보자.\n",
    "\n",
    "\n",
    "아래 코드는 MNIST dataset을 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "C1g79MKHnhsT",
    "outputId": "25221d70-5079-40e1-cd5a-999ca81c7e6b"
   },
   "outputs": [],
   "source": [
    "# training data를 로드 (Load training data)\n",
    "training_set, dataset_info = tfds.load('mnist', split = 'train', as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82QelXpcYdmD"
   },
   "source": [
    "## Inspect the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VjDZz2a90Iyb"
   },
   "source": [
    "training data가 `training_set`로 로드되고 dataset 정보는 `dataset_info`로 로드되었다. `dataset_info`로부터 classes들의 총 개수와 training set에 있는 images의 전체 수를 얻어오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9euLZQJD0g05",
    "outputId": "ba83ff2e-0702-45cb-ba6f-39dad0cb3061"
   },
   "outputs": [],
   "source": [
    "num_classes = dataset_info.features['label'].num_classes\n",
    "print('There are {:,} classes in our dataset'.format(num_classes))\n",
    "\n",
    "num_training_examples = dataset_info.splits['train'].num_examples\n",
    "print('\\nThere are {:,} images in the training set'.format(num_training_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpZNLP-QnyRG"
   },
   "source": [
    "`training_set` 을 iterator로 사용해서 dataset에 loop를 위해서 다음과 같이 구문을 사용한다:\n",
    "\n",
    "```python\n",
    "for image, label in training_set:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "이미지와 labels의 shape와 dtype을 출력해보자. `.take(1)` method를 사용해서 dataset에 있는 하나의 element만 선택한다. dataset은 images들로 구성되어 있으므로 `.take(1)` method는 하나의 image만 선택할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "n1EXyPuDnywu",
    "outputId": "cf377be9-1cff-45e6-cbd4-77b4eec5c2f3"
   },
   "outputs": [],
   "source": [
    "for image, label in training_set.take(1):\n",
    "    print('The images in the training set have:')\n",
    "    print('\\u2022 dtype:', image.dtype) \n",
    "    print('\\u2022 shape:', image.shape)\n",
    "  \n",
    "    print('\\nThe labels of the images have:')\n",
    "    print('\\u2022 dtype:', label.dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcgow6H54ujJ"
   },
   "source": [
    "dataset에서 images들은 `shape = (28, 28, 1)` 와 `dtype = uint8` 의 tensors이다. `unit8` 은 8-bit unsigned integer이고 0~255범위의 정수값을 가질 수 있다. 반면에 images의 label은 `dtype = int64` 의 tensors인데 이말은 64-bit signed integers라는 것을 뜻한다. 이제 dataset에서 하나의 image는 어떤 형태인지 알아보자. images를 그리기 위해서 먼저 TensorFlow tensors로부터 NumPy ndarrays로 `.numpy()` method를 사용해서 변환해야한다. images는 `shape = (28, 28, 1)`를 가지므로 `.squeeze()` method를 사용해서 images를 `shape = (28, 28)` 로 reshape한다. `.squeeze()` method는 ndarray의 shape으로부터 single-dimensional entries를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "z7MooIVjn34f",
    "outputId": "ac9862fb-7f61-426e-c52d-bbf64ebc5c80"
   },
   "outputs": [],
   "source": [
    "for image, label in training_set.take(1):\n",
    "    image = image.numpy().squeeze()\n",
    "    label = label.numpy()\n",
    "    \n",
    "# Plot the image\n",
    "plt.imshow(image, cmap = plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print('The label of this image is:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJkmaOsyYpOz"
   },
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5emVveHn7X6"
   },
   "source": [
    "본바와 같이 images의 pixel 값들은 `[0, 255]` 범위에 있다. images를 정규화시키고 training set으로 pipeline을 생성한다. neural network에 공급할 수 있다. 이미지를 정규화시키기 위해서 pixel 값을 255로 나눈다. 따라서 먼저 image의 `dtype` 을 `tf.cast` function을 사용해서 `uint8` 에서 `float32` 로 변경한다.(32-bit single-precision floating-point numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3g_jqq96n8Ft"
   },
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_batches = training_set.cache().shuffle(num_training_examples//4).batch(batch_size).map(normalize).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXQPznKeoBcx"
   },
   "source": [
    "`64` batch size로 pipeline을 생성했다는 것을 알고 dataset을 섞는다. batch size는 한번의 iteration에서의 images의 개수고 network로 전달된다. 이를 *batch* 라고 부른다. `shuffle` 변환은 network에 feed되기전에 random하게 dataset의 엘리머트를 섞는다.\n",
    "\n",
    "비록 이런 많은 변환은 교환이 가능하며 변환의 순서는 성능에 영향을 미친다. 이런 변환과 성능에 대해서 더 상세한 정보는 아래 링크를 참고하자 :\n",
    "\n",
    "* [Pipeline Performance](https://www.tensorflow.org/beta/guide/data_performance)\n",
    "\n",
    "\n",
    "* [Transformations](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "\n",
    "이제 `training_batches` 를 가지고 이를 살펴보자. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Yh3fw8YvoB_N",
    "outputId": "92cbef35-7d51-40ed-dedd-dd90ccd41fc9"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    print('The images in each batch have:')\n",
    "    print('\\u2022 dtype:', image_batch.dtype) \n",
    "    print('\\u2022 shape:', image_batch.shape)\n",
    "  \n",
    "    print('\\nThere are a total of {} image labels in this batch:'.format(label_batch.numpy().size))\n",
    "    print(label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKyE_NeNoGo5"
   },
   "source": [
    "이제 batches 중에 하나로부터 단일 image를 구하는 방법을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "Td2-DArEoHHu",
    "outputId": "c62e7df4-42bb-41e6-9576-cab6f796680d"
   },
   "outputs": [],
   "source": [
    "# images의 단일 batch를 받아서 squeezing으로 color dimension을 제거 (Take a single batch of images, and remove the color dimension by squeezing it)\n",
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    images = image_batch.numpy().squeeze()\n",
    "    labels = label_batch.numpy()\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(images[0], cmap = plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print('The label of this image is:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aa2qHmjUoMNS"
   },
   "source": [
    "## Build a Simple Neural Network\n",
    "\n",
    "먼저 이 dataset에 weight 행렬과 행렬 곱셈을 이용해서 간단한 network를 생성한다. 이전 notebook에서 했던 것과 동일하다. 다음으로 TensorFlow와 Keras를 사용해서 어떻게 하는지 보도록하자. 이렇게 하면 network 구조를 정의하는 간편하고 강력한 방법을 제공한다.\n",
    "\n",
    "우리가 보왔던 networks들은 *fully-connected* 혹은 *dense* networks라고 불린다. 하나의 layer에서 각 unit은 다음 layer에 있는 각 unit에 연결된다. 완전히 연결된 networks에서 각 layer에 대한 입력은 반드시 1차원 vecotr여야 한다.(2D tensor로 stack이 가능) 하지만 여기서 images는 28 $\\times$ 28 2D tensors 이고 이를 1D vectors로 변환할 필요가 있다. sizes에 대해서 생각하자 shape `(64, 28, 28, 1)` 의 images의 batch를 shape of `(64, 784)`으로 변환할 필요가 있고 784는 28 x 28이다. 이를 일반적으로 *flattening* 이라고 하고 2D images를 1D vectors로 flattened시키는 것이다.\n",
    "\n",
    "이전 notebook에서 하나의 output unit을 가지는 network를 만들었다. 여기서 10개 output units를 필요하고 각 숫자(digit)에 대해서 하나이기 때문이다. 여기서 network이 이미지에 보이는 숫자를 추정하게 하기 위해서 해당 image가 어떤 숫자나 어느 class인지 확률을 계산한다. 해당 숫자(class)에 대한 분산 확률 분산 나와서 해당 이미지가 어떤 숫자인거 같은지를 말해준다. 이 말은 10개 classes(digit)에 대해서 10개 output units를 필요로 한다. network output을 확률 분산으로 변환하는 방법을 보여준다.\n",
    "\n",
    "> **연습문제:** 위에서 우리가 생성한  images의 batch인 `images`를 flatten시킨다. 다음으로 784 input units, 256 hidden units, 10개 output units 에 대해서 weights와 biases를 위한 random tensors를 사용해서 간단한 network를 생성한다. 이제 hidden layer에서 units을 위한 sigmoid activation function을 사용한다. activation 없이 outpout liayer를 남겨둔다. 다음으로 확률 분산을 주는 것을 추가한다. **HINT:** images의 batch를 flatten시키기 위해서 [`tf.reshape()`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/reshape) 를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "69pQ7bEIoMw0",
    "outputId": "edc086d3-29a3-456e-a997-07174c31d87b"
   },
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "output = \n",
    "\n",
    "# output의 shape을 출력 (Print the shape of the output. It should be (64,10))\n",
    "print('The output has shape:', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6YnpZowoSz2"
   },
   "source": [
    "이 network에는 10개 output을 가진다. 하나의 image를 network으로 전달하고 classes에 대해서 확률 분포가 나오며 이것은 해당 이미지가 어느 class와 가장 유사한지를 말해준다. 아래처럼 나타난다 :\n",
    "<img src='assets/image_distribution.png' width=500px>\n",
    "\n",
    "여기서 각 class에 대한 확률은 대략 동일하다. 이는 network이 train되지 않았다는 것을 의마하며 아직ㅈ 어떤 data를 보지 않아서 각 class에 대해서 동일한 확률을 가지는 동일 형태의 분산을 반환한다.\n",
    "\n",
    "이 확률 분포를 계산하는데 [**softmax** function](https://en.wikipedia.org/wiki/Softmax_function) 가 많이 사용된다.\n",
    "수식표현은 아래와 같다\n",
    "\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "\n",
    "이것이 하는 일은 각 input $x_i$ 를 0과 1사이로 squish하고 값들을 적절한 확률 분포로 정규화시켜서 확률의 합이 1이 되도록 한다.\n",
    "\n",
    "> **연습문제:** `softmax` 함수를 구현해보자. 이 함수는 softmax 계산을 수행하고 batch에 있는 각 예제에 대해서 확률 분포를 반환한다. 이를 수행할때 shapes에 대해서 주의를 기울여야 한다. tensor `a` 는 shape `(64, 10)` 를 가지고 tensor `b` 는 shape `(64,)` 가지는 경우 `a/b`를 수행하면 error가 발생한다. 왜냐하면 TensorFlow는 컬럼들에 대해서 division을 수행하려고(broadcasting이라고 불리는) 하지만 size가 맞지 않기 때문이다. 이에 대한 생각 방식은 다음과 같다 : 64개 예제 각각에 대해서, 하나의 값으로만 나누기를 원하는 경우, 분모에 합이 온다. 따라서 `(64, 1)` shape을 가지기 위해서 `b` 가 필요하다. 이런 방식으로 TensorFlow는 `a` 의 각 행에 10개 값으로 나누고 `b` 의 각 행에 하나의 값으로 나눈다. 합을 어떻게 가져올지도 주의해야한다. `tf.reduce_sum()` 에 `axis` 키워드를 정의할 필요가 있다. `axis=0` 설정은 `axis=1` 가 컬럼들에 대해서 합을 가지는 동안 행에 대한 합을 가진다. output tensor가 제대로된 shape `(64,1)`을 가지는 것을 확신하기 위해서는 `tf.reduce_sum()` 에서 `keepdims` 키워드를 사용이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mPtaB817oTTe",
    "outputId": "cada472f-7d69-4581-9d63-d8d62f2f9830"
   },
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "\n",
    "# Apply softmax to the output\n",
    "probabilities = softmax(output)\n",
    "\n",
    "# Print the shape of the probabilities. Should be (64, 10).\n",
    "print('The probabilities have shape:', probabilities.shape, '\\n')\n",
    "\n",
    "\n",
    "# The sum of probabilities for each of the 64 images should be 1\n",
    "sum_all_prob = tf.reduce_sum(probabilities, axis = 1).numpy()\n",
    "\n",
    "# Print the sum of the probabilities for each image.\n",
    "for i, prob_sum in enumerate(sum_all_prob):\n",
    "    print('Sum of probabilities for Image {}: {:.1f}'.format(i+1, prob_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmUrxgU5dK3w"
   },
   "source": [
    "## Building Neural Networks with TensorFlow and Keras\n",
    "\n",
    "Keras는 neural networks를 구성하고 trian시키기 위한 하이레벨 API이다. `tf.keras` 는 Keras API의 TensorFlow의 구현이다. Keras에서 deep learning 모델들은 **layers** 라는 설정 가능한 빌딩 블록들을 연결해서 구성한다. 가장 일반적인 타입의 모델은 **Sequential** 모델이라고 불리는 layers의 stack이다. 이 모델을 sequential이라고 부르는 이유는 각 layer에 operations을 통해서 tensor가 순차적으로 통과되는 것을 허용하기 때문이다. TensorFlow에서 sequential 모델은 `tf.keras.Sequential` 로 구현한다.\n",
    "\n",
    "아래 cell에서 Keras sequential 모델을 사용해서 동일한 완전 연결 neural network을 만든다. 이전 섹션에서 만들어 보았다. 여기서의 sequential 모델은 3개 layers로 구성되어 있다 :\n",
    "\n",
    "* **Input Layer:** `tf.keras.layers.Flatten` — This layer flattens the images by transforming a 2d-array of 28 $\\times$ 28 pixels, to a 1d-array of 784 pixels (28 $\\times$ 28 = 784). The first layer in a Sequential model needs to know the shape of the input tensors to the model. Since, this is our first layer, we need to specify the shape of our input tensors using the `input_shape` argument. The `input_shape` is specified using a tuple that contains the size of our images and the number of color channels. It is important to note that we don't have to include the batch size in the tuple. The tuple can have integers or `None` entries, where `None` entries indicate that any positive integer may be expected.\n",
    "\n",
    "* **Hidden Layer:** `tf.keras.layers.Dense` — A fully-connected (also known as densely connected) layer. For this layer we need to specify the number of neurons (or nodes) we want to use and the activation function. Note that we don't have to specify the shape of the input tensor to this layer, since Keras performs automatic shape inference for all layers except for the first layer. In this particular case, we are going to use `256` neurons with a `sigmoid` activation fucntion. \n",
    "\n",
    "* **Output Layer:** `tf.keras.layers.Dense` — A fully-connected layer with 10 neurons and a *softmax* activation function. The output values will represent the probability that the image is a particular digit. The sum of all the 10 nodes values is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "tujWgGJ1emo_",
    "outputId": "26949733-4eb1-4f57-9d70-a2e39281d755"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "        tf.keras.layers.Dense(256, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGqf8FCZ1bAs"
   },
   "source": [
    "### Your Turn to Build a Neural Network\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "> **연습문제:** 784개 input units, hidden layer는 128개 units 그리고 ReLU activation를 가지며 hidden layer는 64개 units과 ReLU activation을 가지며 마지막으로 output layer는 10개 units과 softmax activation function을 가지고 있다. `activation = 'relu'` 설정으로 ReLU activation function을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "l-s_J0NC1jdH",
    "outputId": "47ce3f16-a287-44df-a0fd-015a5601f29f"
   },
   "outputs": [],
   "source": [
    "## Solution\n",
    "my_model_1 = \n",
    "\n",
    "my_model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdBvHoq5jnkt"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "지금까지 softmax activation을 살펴봤다. 하지만 일반적으로 어떤 함수든 activation function으로 사용될 수 있다. 어떤 network가 비선형 함수를 approximate하기 위한 유일한 요구사항은 activation function은 반드시 비선형이어야만 한다. 일반 activation functions의 몇 가지 추가 예제로 :  Tanh (hyperbolic tangent), and ReLU (rectified linear unit) 가 있다.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "실제로 ReLU function은 hidden layers에 대해서 거의 독보적으로 activation funtion으로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79odRUgEj8_e"
   },
   "source": [
    "## Looking at the Weights and Biases\n",
    "\n",
    "Keras는 자동으로 weights와 biases를 초기화해준다. weights와 biases는 model에 정의한 각 layer에 부착된 tensors이다. `get_weights` method를 사용해서 model로부터 모든 weights와 biases를 얻을 수 있다. `get_weights` method는 NumPy arrays로서 model에서 모든 weight와 bias tensor 목록을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DxZlaMjwe-Lk",
    "outputId": "9992e8d6-0874-49d0-d01c-ab0f21a6c5f3"
   },
   "outputs": [],
   "source": [
    "model_weights_biases = model.get_weights()\n",
    "\n",
    "print(type(model_weights_biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "Zktwcu9ZfAsT",
    "outputId": "ebd43ad2-06ee-4d75-a14e-c1f5f18f236e"
   },
   "outputs": [],
   "source": [
    "print('\\nThere are {:,} NumPy ndarrays in our list\\n'.format(len(model_weights_biases)))\n",
    "\n",
    "print(model_weights_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEOFFR8Yi829"
   },
   "source": [
    "`get_layer` method를 사용해서 특정 layer에 대한 weights와 biases를 얻을 수 있다. 이 경우 전에 했던 것과 `index` argument를 사용해서 먼저 layer를 지정하고 `get_weights` method를 적용한다. 예제로 sequential model의 첫번째 layer의 weights와 biases를 얻기 위해서 다음을 사용한다:\n",
    "\n",
    "```python\n",
    "weights = model.get_layer(index=0).get_weights()[0]\n",
    "biases = model.get_layer(index=0).get_weights()[1]\n",
    "\n",
    "```\n",
    "\n",
    "주의 : model의 첫번째 layer를 가져오기 위해서 사용한 `index=0`은 `tf.keras.layers.Flatten` 이다. 이 layer는 우리 input을 flatten만 하고 weights나 biases는 가지지 않는다. 따라서 이 경우 `index=0`를 가지는 layer는 weights나 biases가 없다. `get_weights()`은 빈 목록 (`[]`)을 반환할 것이기 때문에 `get_weights()[0]` 호출은 error를 생성하게 된다. \n",
    "\n",
    "대안으로 model의 layers의 목록을 가져오기 위해서 `layers` method를 사용할 수도 있다. layers에 대한 loop를 돌리고  `get_weights()` 호출하기 전에 weights를 가지고 있는지를 검사한다. 예제를 한 번 보자.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tcWGSl2rhMif",
    "outputId": "84d94a15-3dec-4567-8bba-874ffdbd0c7f"
   },
   "outputs": [],
   "source": [
    "# Dislay the layers in our model\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Lw1M2CWmfDU3",
    "outputId": "0e3ad837-4ecc-42c5-e10f-12b28635acb5"
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    \n",
    "    if len(layer.get_weights()) > 0:\n",
    "        w = layer.get_weights()[0]\n",
    "        b = layer.get_weights()[1]\n",
    "        \n",
    "        print('\\nLayer {}: {}\\n'.format(i, layer.name))\n",
    "        print('\\u2022 Weights:\\n', w)\n",
    "        print('\\n\\u2022 Biases:\\n', b)\n",
    "        print('\\nThis layer has a total of {:,} weights and {:,} biases'.format(w.size, b.size))\n",
    "        print('\\n------------------------')\n",
    "    \n",
    "    else:\n",
    "        print('\\nLayer {}: {}\\n'.format(i, layer.name))\n",
    "        print('This layer has no weights or biases.')\n",
    "        print('\\n------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8TbtpkxGImY"
   },
   "source": [
    "보는바와 같이 기본적으로 모든 biases는 0으로 초기화된다.\n",
    "\n",
    "반면에 기본적으로 weights들은 Glorot uniform initializer를 통해서 초기화된다. 초기화는 \\[-`limit`, `limit`\\] 내에 uniform distribution으로부터 samples를 그리며 여기서 `limit` 은 `sqrt(6 / (fan_in + fan_out))` 이고 여기서 `fan_in`은 weight tensor에 있는 input units의 개수이고 `fan_out`은 weight tensor에 있는 output units의 개수이다.\n",
    "\n",
    "Keras에서 weights와 biases에 대한 기본 초기화 method를 변경할 수 있다. 유효한 초기화에 대해서 좀더 알고자 한다면 아래 링크는 확인해보자.:\n",
    "\n",
    "* [Available initializers](https://keras.io/initializers/)\n",
    "\n",
    "* [Dense Layer](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyHIkuwBkFTK"
   },
   "source": [
    "## Make Predictions\n",
    "\n",
    "가지고 있는 model로 images batch에 prediction을 하기 위해서 `.predict(image_batch)` method를 사용한다. 이 method는 batch에 images를 가져와서 network에 넣어줘서 전방으로 통과하면서 처리된다. batch에 있는 각 image에 대해서 추정확률을 가지는  shape `(batch_size, num_classes)`의 NumPy ndarray를 출력한다.\n",
    "\n",
    "batch에 대해서 64개 images를 가지고 dataset은 10 classes (*i.e.* `num_classes = 10`) 를 가지므로 model은 shape `(64,10)`의 array를 출력할 것이다. 이 array에 있는 행은 images에 대해서 예측 확률을 가진다. 결과적으로 첫번째 행은 batch에 있는 첫번째 image에 대해서 추정 확률을 가진다. 2번째 행은 batch에 있는 2번째 image에 대한 추정확률을 가진다. 3번째 행은  batch에 있는 3번째 image에 대한 추정확률을 가진다. 이 경우 추정확률은 10개 값으로 구성되어 즉 class당 하나의 확률이 된다. 따라서 batch에 있는 64개 images 각각에 대해서 10개 확률을 가지게 된다. \n",
    "\n",
    "batch에 있는 첫번째 image에 대한 model의 추정 확률을 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "MHMK-x15ju84",
    "outputId": "bac86697-a9d9-4a67-9166-ef53a8e33b2b"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0])\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set_title('Class Probability')\n",
    "ax2.set_xlim(0, 1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nez7TYqwjzN0"
   },
   "source": [
    "위에서 본바와 같이 model은 모든 숫자에 대해서 대략 동일한 확률을 부여한다. 이 말은 우리 network은 기본적으로 image에 있는 해당 숫자가 무엇인지 모른다는 것을 의미한다. 이는 아직 우리 model을 train시키지 않았기 떄문이고 모든 weight는 random이다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHUlGgapoZvf"
   },
   "source": [
    "## Subclassing with TensorFlow and Keras\n",
    "\n",
    "`tf.keras.Sequential` model은 layers의 간단한 stack으로 임의의 model을 생성하는데 사용할 수는 없다. 운좋게 `tf.keras`는 `tf.keras.Model` 을 subclass 및 forward pass를 정의해서 완전한 커스텀 model을 생성하는데 필요한 유연성을 제공한다.\n",
    "\n",
    "다음 예제에서 subclassed `tf.keras.Model`를 사용해서 위에서 만들었던 784 inputs, 256 hidden units, 10 output units의 동일한 nueral network을 만든다. 전과 같이 hidden layer에서 units에 대한 ReLu activation function을 사용하고 output neurons에 대해서 Softmax activation function를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SeLyZ5_oaSz"
   },
   "outputs": [],
   "source": [
    "class Network(tf.keras.Model):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "        # Define layers \n",
    "        self.input_layer = tf.keras.layers.Flatten()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(256, activation = 'relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(self.num_classes, activation = 'softmax')\n",
    "    \n",
    "    # Define forward Pass   \n",
    "    def call(self, input_tensor):\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "    \n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfJjnjIvojTc"
   },
   "source": [
    "하나씩 상세히 알아보자.\n",
    "\n",
    "```python\n",
    "class Network(tf.keras.Model):\n",
    "```\n",
    "\n",
    "`tf.keras.Model`로부터 상속받는다. `super().__init__()`와 결합되어 많은 유용한 methods와 attributes를 제공하는 class를 생성한다. network를 위한 class를 생성할때 `tf.keras.Model`에서 상속받아야만 한다. 하지만 class 이름은 어떤 것이든 상관없다.\n",
    "\n",
    "다음으로 `__init__` method 내부에서 network의 layers를 생성하고 class instance의 attributes로 설정한다. output layer에 neurons의 개수를 할당을 `__init__` method 에서 수행하면 이는  `num_classes` 인자를 통해서 가능하다. 기본값은 2이다.\n",
    "\n",
    "```python\n",
    "self.input = tf.keras.layers.Flatten()\n",
    "```\n",
    "\n",
    "앞에서 말한바와 같이 첫번째 layer는 input image를 flatten한다. 이 layer에  `self.input` 이름을 붙인다. 이후에 이 layer를 참조하는데 이 이름을 사용할 것이다. 여러분의 layer에 어떤 이름을 무엇으로 하는지는 중요하지 않으면 어떤 이름이든 가능하다.\n",
    "\n",
    "```python\n",
    "self.hidden = tf.keras.layers.Dense(256, activation = 'relu')\n",
    "```\n",
    "\n",
    "2번째 layer는 256 neurons와 ReLu activation function을 가지는 완전히 연결된(dense) layer이다. 이 layer에 이름을 `self.hidden`으로 한다. 이 이름으로 이 layer에 대한 참조로 사용한다.\n",
    "\n",
    "```python\n",
    "self.output = tf.keras.layers.Dense(self.num_classes, activation = 'softmax')\n",
    "```\n",
    "\n",
    "3번째와 마지막 layer(output layer)도 `self.num_classes` neurons와 softmax activation function으로 fully-connected (dense) layer이다. 기본적으로 output의 개수는 2가 되지만 여러분의 dataset의 output classes의 개수에 따라서 다른 정수값을 사용할 수 있다.\n",
    "\n",
    "다음으로 `call` method에서 forward pass를 정의한다.\n",
    "\n",
    "```python\n",
    "def call(self, input_tensor):\n",
    "```\n",
    "\n",
    "`tf.keras.Model`로 생성된 TensorFlow models은 반드시 정의된 `call` method를 가져야만 한다. `call` method에서 `input_tensor`를 가지고 `__init__` method에서 정의한 layers로 이것ㅇ르 pass시킨다.\n",
    "\n",
    "```python\n",
    "x = self.input(input_tensor)\n",
    "x = self.hidden(x)\n",
    "x = self.output(x)\n",
    "```\n",
    "\n",
    "`input_tensor`는 각 layer를 통과하고 `x`로 재할당한다. `input_tensor`는 `input` layer에 들어가고 다음으로 `hidden` layer에 들어가고 마지막으로 `output` layer에 들어간다. `__init__` method에서 layers 정의한 순서는 중요하지 않다. 하지만 `call` method에서는 순서가 중요하다. `__init__` method에서 각 layer에 대해서 부여한 이름을 참조한다. 이 이름은 임의로 정한것이라는 것을 명심하자.\n",
    "\n",
    "이제 model class를 정의하고 `model` object를 생성할 수 있다. `Network` class에서 input tensor의 shape을 지정하지 않는다. 이 경우 weights와 biases만 초기화되는데 `build(batch_input_shape)`을 호출해서 model을 만들때나 training/evaluation method (such as `.fit` or `.evaluate`)에 대한 첫번째 호출되는 때이다. 이것을 delayed-build pattern이라고 부른다.\n",
    "\n",
    "따라서 이제는 `model` object를 생성하고 `build()`를 호출해서 만들어보자.(weights와 biases를 초기화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "FZdpLsXioj_w",
    "outputId": "9b8df3b9-a258-4440-82d1-ca2f09b14232"
   },
   "outputs": [],
   "source": [
    "# model 객체 생성(Create a model object)\n",
    "subclassed_model = Network(10)\n",
    "\n",
    "# model 만들기(Build the model, i.e. initialize the model's weights and biases)\n",
    "subclassed_model.build((None, 28, 28, 1))\n",
    "\n",
    "subclassed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvaGttubvdXt"
   },
   "source": [
    "Remember that `None` is used to indicate that any integer may be expected. So, we use `None` to indicate batches of any size are acceptable. \n",
    "\n",
    "While model subclassing offers flexibility, it comes at a cost of greater complexity and more opportunities for\n",
    "user errors. So, we recommend, to always use the simplest tool for the job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYc20VqXo3tm"
   },
   "source": [
    "### Your Turn to Build a Neural Network\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "> **Exercise:** Use the subclassing method to create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with 10 units and a softmax activation function. You can use a ReLU activation function by setting `activation = 'relu'`. After you create your model, create a model object and build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "q4FIQ-BPo1BS",
    "outputId": "ecf18a3b-192f-4b62-a64e-b4abef841080"
   },
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "my_model_2 = \n",
    "\n",
    "my_model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fau1iyME_x1M"
   },
   "source": [
    "## Looking at Weights and Biases of Subclassed Models\n",
    "\n",
    "As before, we can get the weights an biases of each layer in our subclassed models. In this case, we can use the name we gave to each layer in the `__init__` method to get the weights and biases of a particular layer. For example, in the exercise we gave the first hidden layer the name `hidden_1`, so we can get the weights an biases from this layer by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "hdsFjbmRU_el",
    "outputId": "b3183ba2-925d-4d00-c65e-83d5980ba5a6"
   },
   "outputs": [],
   "source": [
    "w1 = my_model_2.hidden_1.get_weights()[0]\n",
    "b1 = my_model_2.hidden_1.get_weights()[1]\n",
    "\n",
    "print('\\n\\u2022 Weights:\\n', w)\n",
    "print('\\n\\u2022 Biases:\\n', b)\n",
    "print('\\nThis layer has a total of {:,} weights and {:,} biases'.format(w1.size, b1.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-y_bv6uBXvs"
   },
   "source": [
    "All the other methods we saw before, such as `.layers`, are also available for subclassed models, so feel free to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAIMbWqEpJuV"
   },
   "source": [
    "## Making Predictions with Subclassed Models\n",
    "\n",
    "Predictions are made in exactly the same way as before. So let's pass an image to our subclassed model and see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "qFV-XUP9pKSn",
    "outputId": "957e6c81-4165-44b9-d0d4-08c3ca1b4742"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = subclassed_model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0])\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set_title('Class Probability')\n",
    "ax2.set_xlim(0, 1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GI-f9IzJpO3A"
   },
   "source": [
    "As before, we can see above, our model gives every digit roughly the same probability. This means our network has basically no idea what the digit in the image is. This is because we haven't trained our model yet, so all the weights are random!\n",
    "\n",
    "In the next notebook, we'll see how we can train a neural network to accurately predict the numbers appearing in the MNIST images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUfsknUC3ctf"
   },
   "source": [
    "## Other Methods to Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "Qlk-03oapl6S",
    "outputId": "ac7ea2e6-db22-411c-b239-446c624e7ad7"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28,1)))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "oBmAM5Ip15Gg",
    "outputId": "ca254c0a-d6bf-45cf-a36c-3365af23d351"
   },
   "outputs": [],
   "source": [
    "layer_neurons = [512, 256, 128, 56, 28, 14]\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28,1)))\n",
    "\n",
    "for neurons in layer_neurons:\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
    "            \n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "          \n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Weo3uwdrA8di"
   },
   "source": [
    "## Clearing the Graph\n",
    "\n",
    "In order to avoid clutter from old models in the graph, we can use:\n",
    "\n",
    "```python\n",
    "tf.keras.backend.clear_session()\n",
    "```\n",
    "\n",
    "This command deletes the current `tf.keras` graph and creates a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "kZ2e667J4Bod",
    "outputId": "6d7882d3-e540-4f5e-8745-e1782c4c4dd3"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "layer_neurons = [512, 256, 128, 56, 28, 14]\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28,1)))\n",
    "\n",
    "for neurons in layer_neurons:\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
    "            \n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "          \n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxZZdnIuA4J2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 2 - Neural networks with TensorFlow and Keras (Solution).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
