{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gRj-x7h332N"
   },
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "이전 파트에서 구성했던 network는 똑똑하지 않다. 왜냐하면 숫자 솔글씨에 대해서 아는 것이 없었다. 비선형 activations를 가지는 Neural network은 universal function approximators처럼 동작한다. 여기에는 input을 output으로 매핑하는 함수가 있다. 예제로 숫자 손글씨의 images를 class 확률로 매핑한다. neural networks의 파워는 훈련시켜서 이 함수가 근사치에 맞게 하고 충분히 주어진 데이터 함수와 계산 시간을 근사치에 맞게 한다.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "먼저 network의 초기에는 inputs을 output으로 매핑하는 함수에 대해서 아는 것이 없다. 실제 데이터의 예제를 보여서 network를 train시키고 다음으로 network parameters를 조정해서 이 함수를 approximates 시킨다.\n",
    "\n",
    "이런 parameters를 찾기 위해서 network가 실제 output과 추정치가 얼마나 다른지를 알아야한다. 이를 위해서 **loss function** (cost 라고 부르는) 를 계산으로 prediction error의 측정한다. 예제로 mean squared loss는 regression과 binary classification 문제에 사용된다.\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "$n$ 는 training 예제의 개수이고 $y_i$ 는 true labels의 개수, $\\hat{y}_i$ 는 추정 labels의 개수이다.\n",
    "\n",
    "network parameters에 대한 이 loss를 최소화시켜서, loss가 최소이고 높은 정확도를 가지고 제대로 labels를 추정할 수 있는 configurations를 찾을 수 있다. **gradient descent** 이라는 프로세스를 사용해서 이 최소값을 찾는다. gradient는 loss function의 slope이고 가장 빠르게 변화되는 방향에 있는 지점이다. 최소 시간에 이 최소값을 얻기 위해서 gradient(downwards)를 따른다. base에 대해서 가장 급경사인 곳을 따라서 산을 내려가는 것을 생각하면 된다.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-bEg-Zz4Q7z"
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "single layer networks에 대해서 gradient descent는 구현이 간단하다. 하지만 우리가 만든 것과 같이 더 deep하고 multilayer neural networks에 대해서는 더 복잡한다. 연구원이 multilayer networks를 train시키는 것을 이해하려면 30년은 족히 걸린다.\n",
    "\n",
    "multilayer networks를 train시키는 것은 **backpropagation** 을 통해서 행해진다. 이는 실제로 미적분에서 chain rule을 단순하게 적용한 것이다. 2 layer network을 graph 표현으로 변환한다고 이해하는게 가장 쉬울 것이다.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "network를 통한 전방 통과에서 data와 operation은 바닥에서 꼭대기로 간다. input $x$ 를 linear transformation $L_1$ 에 weights $W_1$ 와 biases $b_1$ 으로 통과시킨다. 다음으로 output은 sigmoid operation $S$ 와 또 다른 linear transformation $L_2$ 를 수행한다. 마지막으로 loss $\\ell$ 를 계산한다. network의 prediction이 얼마나 안좋은지 측정으로 loss를 사용한다. 다음 목표는 weights와 biases를 조정하여 loss를 최소화시키는 것이다.\n",
    "\n",
    "weights를 gradient descent와 train하기 위해서 loss backwards의 gradient를 network로 전파시킨다. 각 operation은 input과 output 사이에 gradient를 가진다. gradient backwards를 보냄에 따라 incoming gradient를 gradient로 곱한다. 수학적으로 이것은 실제로는 단지 chain rule을 사용해서 weights에 대한 loss의 gradient를 계산하는 것이다.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "**Note:** 여기서 몇가지를 설명 안하고 그냥 넘어갔는데 vector 미적분학에 대한 지식이 필요하지만 무엇을 하고 있는지를 이해하는데는 상관이 없어서 넘어갔다.\n",
    "\n",
    "learning rate $\\alpha$를 가지는 이런 gradient를 사용해서 weights를 업데이트하자.\n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "learning rate $\\alpha$ 는 iterative method가 최소가 되도록 weight update step은 충분히 작게 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "worDfYepJH6j"
   },
   "source": [
    "## Import Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFdhxHwr57Yn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yCtUH8paXqBQ",
    "outputId": "1a4c93cf-21a8-4574-d121-f238912d28e8"
   },
   "outputs": [],
   "source": [
    "print('Using:')\n",
    "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
    "print('\\t\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zQV8MLaJOjN"
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "Att74swb7Ol0",
    "outputId": "a98f6ee1-9881-4d8d-8766-b8b00a2cb4f8"
   },
   "outputs": [],
   "source": [
    "training_set, dataset_info = tfds.load('mnist', split='train', as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IiSe5BPrJquE"
   },
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9r4EMOdT9pM3"
   },
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "num_training_examples = dataset_info.splits['train'].num_examples\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_batches = training_set.cache().shuffle(num_training_examples//4).batch(batch_size).map(normalize).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9SC4gnUJucy"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mo2DfMVvAdbd"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TCpaAlcKCDB"
   },
   "source": [
    "## Getting the Model Ready For Training\n",
    "\n",
    "우리가 만든 model을 train시키기 전에 parameters를 설정해서 train 키는데 사용한다. `.compile` method를 사용해서 train을 위해서 model을 configure할 수 있다. `.compile` method에서 지정이 필요한 main parameters들은 다음과 같다 :\n",
    "\n",
    "* **Optimizer:** training하는 동안 model의 weight를 업데이트하는데 사용하는 알고리즘이다. 이 레슨을 통해서 [`adam`](http://arxiv.org/abs/1412.6980) optimizer를 사용할 예정이다. Adam은 stochastic gradient descent 알고리즘이다. `tf.keras` 에서 유효한 optimizers에 대한 전체 목록은 [optimizers documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers#classes)를 확인하자.\n",
    "\n",
    "\n",
    "* **Loss Function:** The loss function we are going to use during training to measure the difference between the true labels of the images in your dataset and the predictions made by your model. In this lesson we will use the `sparse_categorical_crossentropy` loss function. We use the `sparse_categorical_crossentropy` loss function when our dataset has labels that are integers, and the `categorical_crossentropy` loss function when our dataset has one-hot encoded labels. For a full list of the loss functions available in `tf.keras` check out the [losses documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses#classes).\n",
    "\n",
    "\n",
    "* **Metrics:** A list of metrics to be evaluated by the model during training. Throughout these lessons we will measure the `accuracy` of our model. The `accuracy` calculates how often our model's predictions match the true labels of the images in our dataset. For a full list of the metrics available in `tf.keras` check out the [metrics documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics#classes).\n",
    "\n",
    "이것들이 이 과정을 통해서 설정할 주요 파라미터들이다. [TensorFlow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile)에서 다른 설정 파라미터들을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYv3pv5-InR1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5CjYa8ES3OI"
   },
   "source": [
    "## Taking a Look at the Loss and Accuracy Before Training\n",
    "\n",
    "model을 trian시키기 전에, random weights를 사용할때 model이 어떻게 수행되는지를 살펴보자. images의 single batch를 un-trained model에 전달할때 `loss` 와 `accuracy` values을 살펴보자. 이를 위해서  `.evaluate(data, true_labels)` method를 사용한다. `.evaluate(data, true_labels)` method는 주어진 `data`에 model의 predicted output과 주어진 `true_labels`과 비교하고 `loss` 와 `accuracy` values을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "u_7aijzvJQZ7",
    "outputId": "f66f355e-d030-4c30-e50c-7bba125a20cf"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nLoss before training: {:,.3f}'.format(loss))\n",
    "print('Accuracy before training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvsfbLEMZjZ5"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "training set에 모든 images를 사용하여 model을 train시키자. 전체 dataset을 한번 pass시키는 것을 *epoch*라고 부른다. `.fit` method를 사용해서 epochs의 주어진 숫자에 대한 model 훈련시키며 아래와 같다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Z-CgmnKBZDjq",
    "outputId": "38ab455c-767a-4705-c172-9d7cc926c239"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(training_batches, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFgG_WfUjCic"
   },
   "source": [
    "`.fit` method는 `History` object를 반환하며 여기에는 validation 정확도와 loss values 뿐만 아니라 이후 epochs에 대한 training accuracy 와 loss values의 기록을 포함하고 있다. 이후 과정에서 history object에 대해서 이야기해보자.\n",
    "\n",
    "학습된 model을 가지고 prediciton을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "ghr7z-SnctRw",
    "outputId": "8e946c9a-56b5-45f4-e79f-c6451ff8b7d5"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "  \n",
    "  \n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0])\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set_title('Class Probability')\n",
    "ax2.set_xlim(0, 1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4WcPdR9jKMB"
   },
   "source": [
    "WOW!! 이제 우리 network은 훌륭하다. images에서 숫자를 정확하게 예측할 수 있다. images의 single batch에 대해서 다시 loss와 accuracy values를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "rFZKBfTgfPVy",
    "outputId": "b4d7816a-bbfa-4bb8-c453-82506029aeb8"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nLoss after training: {:,.3f}'.format(loss))\n",
    "print('Accuracy after training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wa5_vwtotNeg"
   },
   "source": [
    "> **연습문제:** 784 input units, a hidden layer with 128 units, 다음으로 a hidden layer with 64 units, 다음으로 a hidden layer with 32 units 그리고 마지막으로 an output layer with 10 units 인 네트워크를 생성하자. 모든 hidden layers에 대해서 ReLu activation function을 그리고 output layer에 대해서는 softmax activation function을 사용하자. 다음으로 `adam` optimizer, `sparse_categorical_crossentropy` loss function, `accuracy` metric 를 사용하여 model을 컴파일한다. image의 single batch에 대한 un-trained model의 loss와 정확도를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "txuSaeuirvgc",
    "outputId": "33af7e60-e292-4788-f168-940351b7e6b4"
   },
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "\n",
    "print('\\nLoss before training: {:,.3f}'.format(loss))\n",
    "print('Accuracy before training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgdaQEVUumxo"
   },
   "source": [
    "> **연습문제:** 위에서 생성한 model을 5 epochs 동안 train시키고 나서 images의 single batch에 대해서 trained model의 loss와 정확도를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "HzxZtgBDt3Ak",
    "outputId": "e487178a-e5dd-411b-e5dc-566983d4aa27"
   },
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "\n",
    "print('\\nLoss after training: {:,.3f}'.format(loss))\n",
    "print('Accuracy after training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfBqrMikvVCY"
   },
   "source": [
    "> **Exercise:** Plot the prediction of the model you created and trained above on a single image from the training set. Also plot the probability predicted by your model for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "NOwMUqYzvKtK",
    "outputId": "5f653945-0fe4-4699-e2cc-98e67e050dbb"
   },
   "outputs": [],
   "source": [
    "## Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqREWxKKVwql"
   },
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "TensorFlow가 계산하고 backpropagation을 위해 필요한 gradients를 추적하는 방법에  대해서 잠시 알아보자. TensorFlow는 자동 미분 연산을 기록하는 class를 제공한다. 그 class는 `tf.GradientTape`이다. 자동 미분은 간단히 \"autodiff\"로 알려져 있지만 컴퓨터에서 수치 함수의 미분을 효과적이고 정확하게 처리하기 위해 사용되는 기술중에 하나다.\n",
    "\n",
    "`tf.GradientTape`은 \"watched\"되는 tensors에서 수행되는 연산을 추적해서 동작한다. 기본적으로 `tf.GradientTape`은 자동으로 어떤 훈련 가능한 변수든 \"watch\"하며 model에서는 weights해 해당된다. 훈련가능한 변수는 `trainable=True`를 가진다. `tf.keras`를 가지는 model을 생성할때 모든 파라미터들은 `trainable = True`로 초기화한다. tensor는 watch method를 호출해서 수동으로 \"watched\"될 수 있다.\n",
    "\n",
    "간단한 예제를 보자. 다음과 같은 식을 따른다.:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "`x`에 대한 `y`의 미분은 다음과 같이 주어진다 :\n",
    "\n",
    "$$\n",
    "\\frac{d y}{d x} = 2x\n",
    "$$\n",
    "\n",
    "`tf.GradientTape`를 사용해서 tensor `x`에 대한 tensor `y`의 미분을 계산하자.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2-ktpx5dVU3O",
    "outputId": "d4a54fba-61eb-4419-e9d9-8162785ef09d"
   },
   "outputs": [],
   "source": [
    "# Set the random seed so things are reproducible\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create a random tensor\n",
    "x = tf.random.normal((2,2))\n",
    "\n",
    "# Calculate gradient\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x ** 2\n",
    "    \n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "# Calculate the actual gradient of y = x^2\n",
    "true_grad = 2 * x\n",
    "\n",
    "# Print the gradient calculated by tf.GradientTape\n",
    "print('Gradient calculated by tf.GradientTape:\\n', dy_dx)\n",
    "\n",
    "# Print the actual gradient of y = x^2\n",
    "print('\\nTrue Gradient:\\n', true_grad)\n",
    "\n",
    "# Print the maximum difference between true and calculated gradient\n",
    "print('\\nMaximum Difference:', np.abs(true_grad - dy_dx).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgLCJaooV5Un"
   },
   "source": [
    "`tf.GradientTape` class는 이런 연산을 유지하고 각각에 대해서 gradient를 계산하는 방법을 알고 있다. 이런 방식으로 어떤 하나의 tensor에 대해서 연산의 chain으로 gradients를 계산하는 것이 가능하다.\n",
    "\n",
    "`tf.GradientTape`와 훈련 가능한 변수에 대해서 상세한 정보는 아래 링크는 참고하자.\n",
    "\n",
    "* [Gradient Tape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape)\n",
    "\n",
    "* [TensorFlow Variables](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Variable)\n",
    "\n",
    "다음에는 좀더 복잡한 dataset을 neural network로 훈련시키기 위한 코드를 작성해볼 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "am0SvU9KWAD3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3 - Training Neural Networks (Solution).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
